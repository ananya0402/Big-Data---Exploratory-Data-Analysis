{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    1.1 Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Write the code to create a SparkContext object using SparkSession, which tells Spark how to access a cluster. To create a SparkSession you first need to build a SparkConf object that contains information about your application. Give an appropriate name for your application and run Spark locally with as many working processors as logical cores on your machine ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on our machine\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment 1\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Import all the “Units” csv files from 2015-2019 into a single RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the units csv files into a single rdd named units_rdd\n",
    "units_rdd = sc.textFile('2015_DATA_SA_Units.csv,2016_DATA_SA_Units.csv,2017_DATA_SA_Units.csv,2018_DATA_SA_Units.csv,2019_DATA_SA_Units.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Import all the “Crashes” csv files from 2015-2019 into a single RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the crashes csv files into a single rdd named crashes_rdd\n",
    "crashes_rdd = sc.textFile('2015_DATA_SA_Crash.csv,2016_DATA_SA_Crash.csv,2017_DATA_SA_Crash.csv,2018_DATA_SA_Crash.csv,2019_DATA_SA_Crash.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. For each Units and Crashes RDDs, remove the header rows and display the total count and first 10 records ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the double quotes(\"\") by nothing otherwise the elements are shown as strings of strings eg.\"'052'\". Thus removing the extra double quotes from the units_rdd.\n",
    "units_rdd = units_rdd.map(lambda x: x.replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of the units_rdd: 153854\n"
     ]
    }
   ],
   "source": [
    "# remove the header row\n",
    "header = units_rdd.first()\n",
    "\n",
    "# the filter method is applied to remove the header rows \n",
    "units_rdd_1 = units_rdd.filter(lambda x: x != header)\n",
    "\n",
    "#Display the count of the rdd\n",
    "print(\"Total count of the units_rdd:\", units_rdd_1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2015-1-21/08/2019,01,0,SA,RIGID TRUCK LGE GE 4.5T,1999,North East,Male,052,SA,HRR ,Full,Not Towing,Swerving,001,5109,,',\n",
       " '2015-1-21/08/2019,02,1,SA,Motor Cars - Sedan,2009,North East,Female,057,SA,C ,Full,Not Towing,Straight Ahead,002,5125,,',\n",
       " '2015-2-21/08/2019,01,0,SA,Motor Cars - Sedan,2009,South East,Male,020,SA,MR,Provisional 1 ,Not Towing,Straight Ahead,001,5110,,',\n",
       " '2015-2-21/08/2019,02,1,SA,Motor Cars - Sedan,1994,South East,Female,021,SA,C ,Full,Not Towing,Stopped on Carriageway,001,5096,,',\n",
       " '2015-3-21/08/2019,01,0,SA,Motor Cars - Sedan,2008,North East,Male,023,SA,C ,Full,Not Towing,Straight Ahead,001,5034,,',\n",
       " '2015-3-21/08/2019,02,1,SA,Motor Cars - Sedan,2007,North East,Female,025,SA,C ,Full,Not Towing,Stopped on Carriageway,001,5015,,',\n",
       " '2015-4-21/08/2019,01,0,SA,Station Wagon,1992,West,Male,059,SA,C R ,Full,Not Towing,Straight Ahead,001,5043,,',\n",
       " '2015-4-21/08/2019,02,0,SA,Motor Cars - Sedan,2009,West,Male,040,SA,C ,Full,Not Towing,Stopped on Carriageway,003,5044,,',\n",
       " '2015-4-21/08/2019,03,0,SA,Motor Cars - Sedan,2006,West,Male,038,SA,C R ,Full,Not Towing,Stopped on Carriageway,001,5022,,',\n",
       " '2015-5-21/08/2019,01,0,,Pedal Cycle,,North,Male,XXX,,,,,Straight Ahead,001,XXXX,,']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show 10 records with the Spark *action* take\n",
    "units_rdd_1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the double quotes(\"\") by nothing otherwise the elements are shown as strings of strings eg.\"'052'\". Thus removing the extra double quotes from the crashes_rdd.\n",
    "crashes_rdd_1 = crashes_rdd.map(lambda x: x.replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of the crashes_rdd: 72006\n"
     ]
    }
   ],
   "source": [
    "# remove the header row\n",
    "crashes_header = crashes_rdd_1.first()\n",
    "# the filter method apply a function to each elemnts. The function output is a boolean value (TRUE or FALSE)\n",
    "# elements that have output TRUE will be kept.\n",
    "crashes_rdd_2 = crashes_rdd_1.filter(lambda x: x != crashes_header)\n",
    "#split columns by a \",\"\n",
    "crashes_rdd_3 = crashes_rdd_2.map(lambda x : x.split(','))\n",
    "#Display the count of the rdd\n",
    "print(\"Total count of the crashes_rdd:\", crashes_rdd_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2015-1-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'ELIZABETH VALE',\n",
       "  '5112',\n",
       "  'CITY OF PLAYFORD.',\n",
       "  '2',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2015',\n",
       "  'January',\n",
       "  'Wednesday',\n",
       "  '01:00 pm',\n",
       "  '060',\n",
       "  'T-Junction',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Side Swipe',\n",
       "  '01',\n",
       "  'Driver Rider',\n",
       "  '2: MI',\n",
       "  'No Control',\n",
       "  '',\n",
       "  '',\n",
       "  '1335254.54',\n",
       "  '1690056.88',\n",
       "  '13352551690057'],\n",
       " ['2015-2-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'SALISBURY',\n",
       "  '5108',\n",
       "  'CITY OF SALISBURY',\n",
       "  '2',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2015',\n",
       "  'February',\n",
       "  'Tuesday',\n",
       "  '03:38 pm',\n",
       "  '060',\n",
       "  'Cross Road',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Rear End',\n",
       "  '01',\n",
       "  'Driver Rider',\n",
       "  '2: MI',\n",
       "  'Traffic Signals',\n",
       "  '',\n",
       "  '',\n",
       "  '1333389.6',\n",
       "  '1688248.34',\n",
       "  '13333901688248'],\n",
       " ['2015-3-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'ST MARYS',\n",
       "  '5042',\n",
       "  'CC MITCHAM.                   ',\n",
       "  '2',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2015',\n",
       "  'March',\n",
       "  'Tuesday',\n",
       "  '01:15 pm',\n",
       "  '070',\n",
       "  'Cross Road',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Rear End',\n",
       "  '01',\n",
       "  'Driver Rider',\n",
       "  '2: MI',\n",
       "  'Traffic Signals',\n",
       "  '',\n",
       "  '',\n",
       "  '1326004.51',\n",
       "  '1661277.67',\n",
       "  '13260051661278'],\n",
       " ['2015-4-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'MITCHELL PARK',\n",
       "  '5043',\n",
       "  'CC MARION.                    ',\n",
       "  '3',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2015',\n",
       "  'January',\n",
       "  'Friday',\n",
       "  '11:15 am',\n",
       "  '060',\n",
       "  'Divided Road',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Rear End',\n",
       "  '01',\n",
       "  'Driver Rider',\n",
       "  '1: PDO',\n",
       "  'No Control',\n",
       "  '',\n",
       "  '',\n",
       "  '1324707.6',\n",
       "  '1660681.57',\n",
       "  '13247081660682'],\n",
       " ['2015-5-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'NORTH BRIGHTON',\n",
       "  '5048',\n",
       "  'CITY OF HOLDFAST BAY',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2015',\n",
       "  'January',\n",
       "  'Sunday',\n",
       "  '06:05 pm',\n",
       "  '050',\n",
       "  'Cross Road',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Rear End',\n",
       "  '01',\n",
       "  'Driver Rider',\n",
       "  '1: PDO',\n",
       "  'Stop Sign',\n",
       "  '',\n",
       "  '',\n",
       "  '1320769.68',\n",
       "  '1661801.51',\n",
       "  '13207701661802'],\n",
       " ['2015-6-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'SALISBURY DOWNS',\n",
       "  '5108',\n",
       "  'CITY OF SALISBURY',\n",
       "  '2',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2015',\n",
       "  'January',\n",
       "  'Monday',\n",
       "  '03:40 pm',\n",
       "  '060',\n",
       "  'Cross Road',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Rear End',\n",
       "  '01',\n",
       "  'Driver Rider',\n",
       "  '2: MI',\n",
       "  'Traffic Signals',\n",
       "  '',\n",
       "  '',\n",
       "  '1331438.94',\n",
       "  '1686827.09',\n",
       "  '13314391686827'],\n",
       " ['2015-7-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'EDWARDSTOWN',\n",
       "  '5039',\n",
       "  'CC MARION.                    ',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2015',\n",
       "  'January',\n",
       "  'Monday',\n",
       "  '06:15 pm',\n",
       "  '050',\n",
       "  'Cross Road',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Right Angle',\n",
       "  '02',\n",
       "  'Driver Rider',\n",
       "  '1: PDO',\n",
       "  'Stop Sign',\n",
       "  '',\n",
       "  '',\n",
       "  '1325689.36',\n",
       "  '1665269.62',\n",
       "  '13256891665270'],\n",
       " ['2015-8-21/08/2019',\n",
       "  '1 City',\n",
       "  'NORTH ADELAIDE',\n",
       "  '5006',\n",
       "  'CITY OF ADELAIDE',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2015',\n",
       "  'January',\n",
       "  'Tuesday',\n",
       "  '12:30 pm',\n",
       "  '050',\n",
       "  'Not Divided',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Rear End',\n",
       "  '01',\n",
       "  'Driver Rider',\n",
       "  '1: PDO',\n",
       "  'No Control',\n",
       "  '',\n",
       "  '',\n",
       "  '1328296.1',\n",
       "  '1671670.41',\n",
       "  '13282961671670'],\n",
       " ['2015-9-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'PLYMPTON',\n",
       "  '5038',\n",
       "  'CITY OF WEST TORRENS',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2015',\n",
       "  'January',\n",
       "  'Monday',\n",
       "  '07:30 pm',\n",
       "  '060',\n",
       "  'Divided Road',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Rear End',\n",
       "  '02',\n",
       "  'Driver Rider',\n",
       "  '1: PDO',\n",
       "  'No Control',\n",
       "  '',\n",
       "  '',\n",
       "  '1324317.73',\n",
       "  '1666625.75',\n",
       "  '13243181666626'],\n",
       " ['2015-10-21/08/2019',\n",
       "  '2 Metropolitan',\n",
       "  'NORWOOD',\n",
       "  '5067',\n",
       "  'CC OF NORWOOD',\n",
       "  'PAYNEHAM & ST PETERS',\n",
       "  '2',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2015',\n",
       "  'January',\n",
       "  'Thursday',\n",
       "  '02:35 pm',\n",
       "  '050',\n",
       "  'Cross Road',\n",
       "  'Straight road',\n",
       "  'Level',\n",
       "  'Not Applicable',\n",
       "  'Sealed',\n",
       "  'Dry',\n",
       "  'Not Raining',\n",
       "  'Daylight',\n",
       "  'Rear End',\n",
       "  '02',\n",
       "  'Driver Rider',\n",
       "  '2: MI',\n",
       "  'Traffic Signals',\n",
       "  '',\n",
       "  '',\n",
       "  '1331854.38',\n",
       "  '1671072.44',\n",
       "  '13318541671072']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show 10 records of the crashes_rdd_2 with the Spark *action* take\n",
    "crashes_rdd_3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Partitioning in RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. How many partitions do the above RDDs have? How is the data in these RDDs partitioned by default, when we do not explicitly specify any partitioning strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default partitions of the units_rdd:  5\n"
     ]
    }
   ],
   "source": [
    "print('Default partitions of the units_rdd: ',units_rdd_1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default partitions of the crashes_rdd:  5\n"
     ]
    }
   ],
   "source": [
    "print('Default partitions of the crashes_rdd: ',crashes_rdd_1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default when we do not explicitly specify any partitioning strategy, Spark partitions the data using <strong>Random equal partitioning</strong> unless there are specific transformations that uses a different type of partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. In the “Units” csv dataset, there is a column called Lic State which shows the state where the vehicle is registered. Assume we want to keep all the data related to SA in one partition and the rest of the data in another partition.\n",
    "###### a. Create a Key Value Pair RDD with Lic State as the key and rest of the other columns as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement function with logic to be applied to the RDDs\n",
    "def parseRecord(line):\n",
    "    # Split line separated by comma\n",
    "    array_line = line.split(',')\n",
    "    # Return a tuple with the Lic State as first element and the remaining as the second element\n",
    "    return (array_line[9], array_line[0:8]+array_line[10:])\n",
    "\n",
    "#The key value pair RDD with Lic State as the key is stored in a new rdd named as units_rdd_2\n",
    "units_rdd_2 = units_rdd_1.map(parseRecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b. Write the code to implement this partitioning in RDD using appropriate partitioning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "\n",
    "#A Function to print the data items in each RDD\n",
    "def print_partitions(data):\n",
    "    if isinstance(data, RDD):\n",
    "        numPartitions = data.getNumPartitions()\n",
    "        partitions = data.glom().collect()\n",
    "    else:\n",
    "        numPartitions = data.rdd.getNumPartitions()\n",
    "        partitions = data.rdd.glom().collect()\n",
    "    \n",
    "    \n",
    "    for index, partition in enumerate(partitions):\n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hash Function to implement Hash Partitioning \n",
    "#The hash function used states that when the key is SA, the total is 2 else it is 3\n",
    "def hash_function(key):\n",
    "    total=0\n",
    "    if(key=='SA'):\n",
    "        total=2\n",
    "    else:\n",
    "        total=3\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the number of partitions\n",
    "no_of_partitions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash partitioning\n",
    "#As the number of partitions is 2, if the total is 2 i.e. when key is SA => 2%2 = 0 => It goes in partition 0 else 3%2 = 1 => It goes in partition 1.\n",
    "hash_partitioned_rdd = units_rdd_2.partitionBy(no_of_partitions, hash_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### c. Write the code to print the number of records in each partition. What does it tell about the data skewness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partition 0: 109684 records\n",
      "Partition 1: 44170 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: {}\".format(hash_partitioned_rdd.getNumPartitions()))\n",
    "print_partitions(hash_partitioned_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is spread unevenly i.e. it is skewed after partitioning with 109684 records in partition 0 and 44170 records in partition 1. Unevenness of data placement is caused by the fact that data value distribution, which is used in the data partitioning function,may well be nonuniform because of the nature of data value distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Query/Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Find the average age of male and female drivers separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement function with logic to be applied to the RDDs\n",
    "def parseRecord(line):\n",
    "    # Split line separated by comma\n",
    "    array_line = line.split(',')\n",
    "    # Return a tuple with the car model as first element and the remaining as the second element\n",
    "    return (array_line)\n",
    "\n",
    "units_rdd_1 = units_rdd_1.map(parseRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A filter method is applied to keep only those records of people with sex as male.\n",
    "male_units_rdd = units_rdd_1.filter(lambda x: x[7]=='Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A filter method is applied to keep only those records of people with sex as female.\n",
    "female_units_rdd = units_rdd_1.filter(lambda x: x[7]=='Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male drivers' average age: 40.98\n"
     ]
    }
   ],
   "source": [
    "male_avg_age = male_units_rdd.map(lambda x : x[8]).filter(lambda x : x.isdigit()).map(lambda x: int(x)).mean()\n",
    "print(\"Male drivers' average age: \"+str(round(male_avg_age,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female drivers' average age: 40.39\n"
     ]
    }
   ],
   "source": [
    "female_avg_age = female_units_rdd.map(lambda x : x[8]).filter(lambda x : x.isdigit()).map(lambda x: int(x)).mean()\n",
    "print(\"Female drivers' average age: \"+str(round(female_avg_age,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. What is the oldest and the newest vehicle year involved in the accident? Display the Registration State, Year and Unit type of the vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newest vehicle year: 2019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('SA', 'Station Wagon', '2019'),\n",
       " ('SA', 'OMNIBUS', '2019'),\n",
       " ('SA', 'Motor Cars - Sedan', '2019'),\n",
       " ('SA', 'Station Wagon', '2019'),\n",
       " ('SA', 'SEMI TRAILER', '2019'),\n",
       " ('SA', 'Motor Cars - Sedan', '2019'),\n",
       " ('SA', 'Motor Cars - Sedan', '2019'),\n",
       " ('VIC', 'Station Wagon', '2019'),\n",
       " ('SA', 'Station Wagon', '2019'),\n",
       " ('SA', 'Utility', '2019')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Newest vehicle year is the year with the maximum in the vehicle year column\n",
    "newest_veh_year = units_rdd_1.map(lambda x : x[5]).filter(lambda x : x.isdigit()).map(lambda x: int(x)).max()\n",
    "print(\"Newest vehicle year: \" +str(newest_veh_year))\n",
    "#Display the details of the vehicle with the newest vehicle year\n",
    "newest_year_details = units_rdd_1.filter(lambda x: x[5]==str(newest_veh_year)).map(lambda field: (field[3],field[4],field[5]))\n",
    "newest_year_details.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oldest vehicle year: 1900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('VIC', 'Motor Cycle', '1900'),\n",
       " ('SA', 'Motor Cycle', '1900'),\n",
       " ('SA', 'Motor Cycle', '1900'),\n",
       " ('SA', 'Motor Cycle', '1900'),\n",
       " ('SA', 'Motor Cycle', '1900'),\n",
       " ('SA', 'Motor Cycle', '1900'),\n",
       " ('SA', 'Motor Cycle', '1900'),\n",
       " ('SA', 'RIGID TRUCK LGE GE 4.5T', '1900'),\n",
       " ('SA', 'Motor Cycle', '1900'),\n",
       " ('SA', 'Motor Cycle', '1900')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Oldest vehicle year is the year with the maximum in the vehicle year column\n",
    "oldest_veh_year = units_rdd_1.map(lambda x : x[5]).filter(lambda x : x.isdigit()).map(lambda x: int(x)).min()\n",
    "print(\"Oldest vehicle year: \" +str(oldest_veh_year))\n",
    "#Display the details of the vehicle with the oldest vehicle year\n",
    "oldest_year_details = units_rdd_1.filter(lambda x: x[5]==str(oldest_veh_year)).map(lambda field: (field[3],field[4],field[5]))\n",
    "oldest_year_details.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Working with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Load all units and crash data into two separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the units csv files in a units_df dataframe\n",
    "units_df = spark.read.csv(\"*_DATA_SA_Units.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the crashes csv files in a crashes_df dataframe\n",
    "crashes_df = spark.read.csv(\"*_DATA_SA_Crash.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Display the schema of the final two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Unit No: integer (nullable = true)\n",
      " |-- No Of Cas: integer (nullable = true)\n",
      " |-- Veh Reg State: string (nullable = true)\n",
      " |-- Unit Type: string (nullable = true)\n",
      " |-- Veh Year: string (nullable = true)\n",
      " |-- Direction Of Travel: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Lic State: string (nullable = true)\n",
      " |-- Licence Class: string (nullable = true)\n",
      " |-- Licence Type: string (nullable = true)\n",
      " |-- Towing: string (nullable = true)\n",
      " |-- Unit Movement: string (nullable = true)\n",
      " |-- Number Occupants: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- Rollover: string (nullable = true)\n",
      " |-- Fire: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Schema for units_df\n",
    "units_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Stats Area: string (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Postcode: integer (nullable = true)\n",
      " |-- LGA Name: string (nullable = true)\n",
      " |-- Total Units: integer (nullable = true)\n",
      " |-- Total Cas: integer (nullable = true)\n",
      " |-- Total Fats: integer (nullable = true)\n",
      " |-- Total SI: integer (nullable = true)\n",
      " |-- Total MI: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Area Speed: integer (nullable = true)\n",
      " |-- Position Type: string (nullable = true)\n",
      " |-- Horizontal Align: string (nullable = true)\n",
      " |-- Vertical Align: string (nullable = true)\n",
      " |-- Other Feat: string (nullable = true)\n",
      " |-- Road Surface: string (nullable = true)\n",
      " |-- Moisture Cond: string (nullable = true)\n",
      " |-- Weather Cond: string (nullable = true)\n",
      " |-- DayNight: string (nullable = true)\n",
      " |-- Crash Type: string (nullable = true)\n",
      " |-- Unit Resp: integer (nullable = true)\n",
      " |-- Entity Code: string (nullable = true)\n",
      " |-- CSEF Severity: string (nullable = true)\n",
      " |-- Traffic Ctrls: string (nullable = true)\n",
      " |-- DUI Involved: string (nullable = true)\n",
      " |-- Drugs Involved: string (nullable = true)\n",
      " |-- ACCLOC_X: double (nullable = true)\n",
      " |-- ACCLOC_Y: double (nullable = true)\n",
      " |-- UNIQUE_LOC: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Schema for crashes_df\n",
    "crashes_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Query/Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Find all the crash events in Adelaide where the total number of casualties in the event is more than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|Stats Area|  Suburb|Postcode|        LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|     Day|    Time|Area Speed|Position Type|Horizontal Align|Vertical Align|          Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight|    Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2018-601-17/01/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          8|        4|         0|       2|       2|2018| January|  Sunday|09:12 pm|        50|  Not Divided|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|   Night|Hit Pedestrian|        1|Driver Rider|        3: SI|     No Control|        null|          null|1329806.36|1670224.76|13298061670225|\n",
      "|2017-1613-15/08/2019|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          2|        4|         0|       0|       4|2017|February|Saturday|04:00 pm|        50|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|        1|Driver Rider|        2: MI|Traffic Signals|        null|          null|1327951.24|1669556.92|13279511669557|\n",
      "|2017-12182-15/08/...|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          6|        5|         0|       1|       4|2017|December|Saturday|04:08 pm|        50|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Wet| Not Raining|Daylight|Hit Pedestrian|        1|Driver Rider|        3: SI|Traffic Signals|        null|          null| 1329016.2|1670995.07|13290161670995|\n",
      "|2019-10404-8/07/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          4|        6|         0|       0|       6|2019| October|  Monday|08:20 am|        60| Divided Road|   Straight road|         Level|Driveway or Entrance|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|        1|Driver Rider|        2: MI|     No Control|        null|          null|1327088.72|1670880.07|13270891670880|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A filter method is applied to get only the rows with Suburb as Adelaide and total casualities more than 3\n",
    "crashes_df.filter(col(\"Suburb\") == 'ADELAIDE')\\\n",
    "                .filter(col(\"Total Cas\")>3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Display 10 crash events with highest casualties ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|    Stats Area|         Suburb|Postcode|            LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|      Day|    Time|Area Speed|Position Type|    Horizontal Align|Vertical Align|    Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight| Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2017-288-15/08/2019|2 Metropolitan|     PARA HILLS|    5096|   CITY OF SALISBURY|          2|       11|         0|       1|      10|2017| January|Wednesday|01:13 pm|        60|   T-Junction|       Straight road| Crest of Hill|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|Right Angle|        1|Driver Rider|        3: SI|      Stop Sign|        null|          null| 1334428.9|1683032.96|13344291683033|\n",
      "|2016-3035-15/08/2019|2 Metropolitan|        HACKHAM|    5163| CITY OF ONKAPARINGA|          3|        9|         3|       5|       1|2016| January| Saturday|11:50 am|        80|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|        1|Driver Rider|     4: Fatal|     No Control|        null|          null|1320361.49|1645195.63|13203611645196|\n",
      "|2016-6630-15/08/2019|2 Metropolitan|  KANGAROO FLAT|    5118|LIGHT REGIONAL CO...|          3|        9|         0|       2|       7|2016|   April|Wednesday|09:00 pm|       100|  Not Divided|CURVED, VIEW OBSC...|         Level|Not Applicable|      Sealed|          Dry| Not Raining|   Night|    Head On|        1|Driver Rider|        3: SI|     No Control|        null|          null|1339316.32|1710314.92|13393161710315|\n",
      "|2019-11734-8/07/2020|2 Metropolitan|          STURT|    5047|CC MARION.       ...|          2|        9|         0|       1|       8|2019|November|   Sunday|07:25 pm|        60|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|        2|Driver Rider|        3: SI|Traffic Signals|        null|          null|1324428.84|1659884.95|13244291659885|\n",
      "|2016-14407-15/08/...|     3 Country|      STOCKWELL|    5355|THE BAROSSA COUNCIL.|          2|        8|         1|       6|       1|2016| October|   Sunday|03:46 pm|       100|  Not Divided|       Straight road| Crest of Hill|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|    Head On|        1|Driver Rider|     4: Fatal|     No Control|        null|          null|1373964.45|1723462.57|13739641723463|\n",
      "|2016-7073-15/08/2019|     3 Country|       MERRITON|    5523|PT.PIRIE CITY & D...|          2|        8|         4|       3|       1|2016|   April|   Sunday|12:35 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|        1|Driver Rider|     4: Fatal|     No Control|        null|          null|1293759.89|1840109.96|12937601840110|\n",
      "|2015-2823-21/08/2019|     3 Country|         HAWKER|    5434|THE FLINDERS RANG...|          1|        8|         0|       0|       8|2015|   March|   Monday|06:00 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|  Roll Over|        1|Driver Rider|        2: MI|     No Control|        null|          null|1315077.61|2022309.34|13150782022309|\n",
      "|2015-12591-21/08/...|     3 Country|        MALLALA|    5502|DC MALLALA.      ...|          2|        7|         0|       2|       5|2015| October|   Sunday|02:30 pm|       100|   Cross Road|       Straight road|         Level|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|Right Angle|        1|Driver Rider|        3: SI|  Give Way Sign|        null|          null|1325122.01|1724860.95|13251221724861|\n",
      "|2015-13713-21/08/...|2 Metropolitan|ELIZABETH GROVE|    5112|   CITY OF PLAYFORD.|          2|        7|         0|       0|       7|2015|November|   Friday|03:42 pm|        80|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|   Rear End|        1|Driver Rider|        2: MI|     No Control|        null|          null|1336118.68|1691385.65|13361191691386|\n",
      "|2015-6965-21/08/2019|     3 Country|       BEAUFORT|    5550|YORKE PENINSULA C...|          3|        7|         3|       4|       0|2015|    June|   Monday|11:13 am|       100|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|        9|       Other|     4: Fatal|     No Control|        null|          null|1287930.19|1761652.36|12879301761652|\n",
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#10 events with highest casualities is shown by sorting the dataframe\n",
    "crashes_df_desc = crashes_df.select(\"*\").sort('Total Cas', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Find the total number of fatalities for each crash type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#The crashes_df is grouped by the crash type and then the fatalities are counted for each of the crash type and then the data is sorted in the descending order\n",
    "agg_attribute = 'Crash Type'\n",
    "sum_attribute = 'Total Fats'\n",
    "crashes_fats_df = crashes_df.groupby(agg_attribute).agg(F.sum(sum_attribute).alias('Total Fatalities')).sort('Total Fatalities', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|          Crash Type|Total Fatalities|\n",
      "+--------------------+----------------+\n",
      "|    Hit Fixed Object|             152|\n",
      "|             Head On|              86|\n",
      "|      Hit Pedestrian|              70|\n",
      "|           Roll Over|              57|\n",
      "|         Right Angle|              45|\n",
      "|          Side Swipe|              20|\n",
      "|          Right Turn|              18|\n",
      "|            Rear End|              16|\n",
      "|  Hit Parked Vehicle|               9|\n",
      "|          Hit Animal|               4|\n",
      "|  Hit Object on Road|               2|\n",
      "|               Other|               2|\n",
      "|Left Road - Out o...|               1|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crashes_fats_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. Find the total number of casualties for each suburb when the vehicle was driven by an unlicensed driver. You are required to display the name of the suburb and the total number of casualties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the crashes and the units dataframes are joined on the Report_ID column \n",
    "crashes_units_df = units_df.join(crashes_df,units_df['Report_ID']==crashes_df['Report_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#The joined dataframe is filtered for the unlicenced licence type and then grouped by suburb and then the casualities are calculated for each suburb and the data is sorted in descending order.\n",
    "agg_attribute = 'Suburb'\n",
    "sum_attribute = 'Total Cas'\n",
    "crashes_units_cas_df = crashes_units_df.filter(col(\"Licence Type\") == 'Unlicenced').groupby(agg_attribute).agg(F.sum(sum_attribute).alias('Total Casualities')).sort('Total Casualities', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|         Suburb|Total Casualities|\n",
      "+---------------+-----------------+\n",
      "|       ADELAIDE|               19|\n",
      "|      DRY CREEK|               18|\n",
      "|      SALISBURY|               18|\n",
      "| SALISBURY EAST|               16|\n",
      "|       PROSPECT|               14|\n",
      "| NORTH ADELAIDE|               13|\n",
      "|   ANDREWS FARM|               12|\n",
      "|        ENFIELD|               12|\n",
      "|SALISBURY SOUTH|               11|\n",
      "|SALISBURY DOWNS|               11|\n",
      "|   BEDFORD PARK|               11|\n",
      "|     INGLE FARM|               11|\n",
      "|     MUNNO PARA|               10|\n",
      "|         BURTON|               10|\n",
      "|   MOUNT BARKER|               10|\n",
      "+---------------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crashes_units_cas_df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Severity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Find the total number of crash events for each severity level. Which severity level is the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#Aggregate the data by security level and count the total fatalities for each severity level\n",
    "agg_attribute = 'CSEF Severity'\n",
    "sum_attribute = 'Total Fats'\n",
    "crashes_fats_df = crashes_df.groupby(agg_attribute).agg(F.count(agg_attribute).alias('Total Crash Events')).sort('Total Crash events', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|CSEF Severity|Total Crash Events|\n",
      "+-------------+------------------+\n",
      "|       1: PDO|             46696|\n",
      "|        2: MI|             21881|\n",
      "|        3: SI|              2978|\n",
      "|     4: Fatal|               451|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crashes_fats_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1:PDO i.e. the Property Damage Only is the most common severity level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Compute the total number of crash events for each severity level and the percentage for the four different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a. When the driver is tested positive on drugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#Aggregate the data by security level and count the total crash events for each severity level \n",
    "agg_attribute = 'CSEF Severity'\n",
    "sum_attribute = 'Drugs Involved'\n",
    "crashes_drugs_df = crashes_df.groupby(agg_attribute).agg(count(when(col(\"Drugs Involved\") == 'Y',1)).alias('Count'))\n",
    "#total number of crash events\n",
    "total = crashes_drugs_df.select(F.sum('Count')). collect()[0][0]                       \n",
    "#Add the percentage column when driver is tested positive on drugs to the dataframe\n",
    "crashes_drugs_df = crashes_drugs_df.withColumn('Percentage',col('Count')*100/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+------------------+\n",
      "|CSEF Severity|Count|        Percentage|\n",
      "+-------------+-----+------------------+\n",
      "|     4: Fatal|   82| 6.539074960127592|\n",
      "|        2: MI|  749|59.728867623604465|\n",
      "|       1: PDO|  176|14.035087719298245|\n",
      "|        3: SI|  247|19.696969696969695|\n",
      "+-------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crashes_drugs_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b. When the driver is tested positive for blood alcohol concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#Aggregate the data by security level and count the total crash events for each severity level \n",
    "agg_attribute = 'CSEF Severity'\n",
    "sum_attribute = 'DUI Involved'\n",
    "crashes_dui_df = crashes_df.groupby(agg_attribute).agg(count(when(col(\"DUI Involved\") == 'Y',1)).alias('Count'))\n",
    "#total number of crash events\n",
    "total = crashes_dui_df.select(F.sum('Count')). collect()[0][0]                       \n",
    "#Add the percentage column when driver is tested positive on alcohol to the dataframe\n",
    "crashes_dui_df = crashes_dui_df.withColumn('Percentage',col('Count')*100/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+------------------+\n",
      "|CSEF Severity|Count|        Percentage|\n",
      "+-------------+-----+------------------+\n",
      "|     4: Fatal|   79|  3.51423487544484|\n",
      "|        2: MI|  737|  32.7846975088968|\n",
      "|       1: PDO| 1173|52.179715302491104|\n",
      "|        3: SI|  259| 11.52135231316726|\n",
      "+-------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crashes_dui_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### c. When the driver is tested positive for both drugs and blood alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "agg_attribute = 'CSEF Severity'\n",
    "#Aggregate the data by security level and count the total crash events for each severity level \n",
    "crashes_drugs_dui_df = crashes_df.groupby(agg_attribute).agg(count(when((col(\"Drugs Involved\") == \"Y\") & (col(\"DUI Involved\") == \"Y\"),1)).alias('Count'))\n",
    "#total number of crash events\n",
    "total = crashes_drugs_dui_df.select(F.sum('Count')). collect()[0][0] \n",
    "#Add the percentage column when driver is tested positive on drugs and alcohol to the dataframe\n",
    "crashes_drugs_dui_df = crashes_drugs_dui_df.withColumn('Percentage',col('Count')*100/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+------------------+\n",
      "|CSEF Severity|Count|        Percentage|\n",
      "+-------------+-----+------------------+\n",
      "|     4: Fatal|   27|15.428571428571429|\n",
      "|        2: MI|   89|50.857142857142854|\n",
      "|       1: PDO|   24|13.714285714285714|\n",
      "|        3: SI|   35|              20.0|\n",
      "+-------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crashes_drugs_dui_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### d. When the driver is tested negative for both (no alcohol and no drugs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#Aggregate the data by security level and count the total crash events for each severity level \n",
    "agg_attribute = 'CSEF Severity'\n",
    "crashes_drugs_dui_neg_df = crashes_df.groupby(agg_attribute).agg(count(when((col(\"Drugs Involved\").isNull()) & (col(\"DUI Involved\").isNull()),1)).alias('Count'))\n",
    "#total number of crash events\n",
    "total = crashes_drugs_dui_neg_df.select(F.sum('Count')). collect()[0][0]                       \n",
    "#Add the percentage column when driver is tested negative on drugs and alcohol to the dataframe\n",
    "crashes_drugs_dui_neg_df = crashes_drugs_dui_neg_df.withColumn('Percentage',col('Count')*100/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+------------------+\n",
      "|CSEF Severity|Count|        Percentage|\n",
      "+-------------+-----+------------------+\n",
      "|     4: Fatal|  317|0.4615675825215859|\n",
      "|        2: MI|20484|29.825710916000524|\n",
      "|       1: PDO|45371| 66.06240626683557|\n",
      "|        3: SI| 2507|3.6503152346423215|\n",
      "+-------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crashes_drugs_dui_neg_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compare the results in these 4 scenarios. Briefly explain the observation from this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1:PDO i.e. Property Damage Only is the most common when the driver is on alcohol or is sober i.e. not under the influence of drugs or alcohol. 2:MI i.e. Minor Injury is prevalent when the driver is on drugs or both i.e. on drugs as well as alcohol. A 3: SI severity level of severe injury is caused mostly when the driver is under the influence of drugs or under the influence of both drugs and alcohol. Crashes lead to fatality i.e. severity level 4 the maximum times when the driver is under the influence of both drugs and alcohol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 RDDs vs DataFrame vs Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Find the Date1 and Time of Crash, Number of Casualties in each unit and the Gender, Age, License Type of the unit driver for the suburb \"Adelaide\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RDD Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58 ms, sys: 9.16 ms, total: 67.1 ms\n",
      "Wall time: 4.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['2015-March-Saturday', '08:00 pm', '0', 'Male', '042', 'Full'],\n",
       " ['2015-October-Tuesday', '10:00 am', '0', 'Male', '024', 'Full'],\n",
       " ['2015-October-Tuesday', '10:00 am', '0', '', '', ''],\n",
       " ['2015-October-Tuesday', '10:00 am', '0', '', '', ''],\n",
       " ['2015-October-Thursday', '01:10 pm', '0', 'Female', '040', ''],\n",
       " ['2015-October-Thursday', '01:10 pm', '0', 'Female', 'XXX', 'Unknown'],\n",
       " ['2015-December-Friday', '05:50 pm', '0', 'Male', '065', 'Full'],\n",
       " ['2015-December-Friday', '05:50 pm', '0', 'Female', '035', 'Full'],\n",
       " ['2015-February-Saturday', '04:00 pm', '1', 'Female', '030', 'Full'],\n",
       " ['2015-February-Saturday', '04:00 pm', '0', 'Male', '049', 'Full']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#get the columns needed from the units_rdd\n",
    "units_details = units_rdd_1.map(lambda x: (x[0], [x[2], x[7], x[8], x[11]]))\n",
    "#filter method is applied to suburbs that are Adelaide and then date, time and total casualities are mapped from the carshes_rdd \n",
    "crashes_adelaide_details = crashes_rdd_3.filter(lambda x: x[0:][2] == 'ADELAIDE').map(lambda x: (x[0],[x[10]+'-'+x[11]+'-'+x[12],x[13]]))\n",
    "#Both the rdd's having the required details are joined\n",
    "crashes_units_details_rdd = units_details.join(crashes_adelaide_details)\n",
    "crashes_units_details_rdd = crashes_units_details_rdd.map(lambda x:  [x[1][1][0], x[1][1][1], x[1][0][0], x[1][0][1], x[1][0][2], x[1][0][3]])\n",
    "#print the top 10 rows\n",
    "crashes_units_details_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataframe approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+---------+-------+----+--------------+\n",
      "|Date               |Time    |No of Cas|Gender |Age |Licence Type  |\n",
      "+-------------------+--------+---------+-------+----+--------------+\n",
      "|2015-April-Friday  |03:44 pm|0        |null   |null|null          |\n",
      "|2015-April-Friday  |03:44 pm|1        |Male   |016 |null          |\n",
      "|2015-April-Friday  |03:44 pm|0        |null   |null|null          |\n",
      "|2015-April-Monday  |02:22 pm|0        |Male   |040 |Unlicenced    |\n",
      "|2015-April-Monday  |12:03 pm|0        |Male   |049 |Full          |\n",
      "|2015-April-Monday  |09:05 am|1        |Male   |067 |Full          |\n",
      "|2015-April-Monday  |09:05 am|0        |Male   |036 |Full          |\n",
      "|2015-April-Monday  |02:22 pm|0        |Female |045 |Provisional 1 |\n",
      "|2015-April-Monday  |12:03 pm|0        |Male   |040 |Full          |\n",
      "|2015-April-Monday  |03:15 pm|0        |Male   |027 |Full          |\n",
      "|2015-April-Monday  |03:15 pm|0        |Unknown|XXX |Unknown       |\n",
      "|2015-April-Monday  |03:15 pm|1        |Male   |063 |Full          |\n",
      "|2015-April-Saturday|12:40 pm|0        |Unknown|XXX |Unknown       |\n",
      "|2015-April-Saturday|01:10 am|0        |Male   |042 |Full          |\n",
      "|2015-April-Saturday|01:10 am|0        |Male   |024 |Full          |\n",
      "|2015-April-Saturday|08:10 am|0        |Male   |042 |Unlicenced    |\n",
      "|2015-April-Saturday|08:10 am|1        |Female |029 |Full          |\n",
      "|2015-April-Saturday|12:40 pm|0        |null   |null|null          |\n",
      "|2015-April-Sunday  |07:00 pm|1        |Female |020 |Full          |\n",
      "|2015-April-Sunday  |01:30 pm|0        |Male   |025 |Unknown       |\n",
      "+-------------------+--------+---------+-------+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 27.2 ms, sys: 2.81 ms, total: 30.1 ms\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#join the crashes and the units dataframes on the report id column\n",
    "crashes_units_df = crashes_df.join(units_df,units_df['Report_ID'] == crashes_df['Report_ID'])\n",
    "#Filter the rows with suburb as Adelaide and select the gender, age, licence type, number of casualities\n",
    "crashes_by_unit_df =  crashes_units_df.filter(col(\"Suburb\") == \"ADELAIDE\")\n",
    "crashes_by_unit_df = crashes_by_unit_df.select(concat(col(\"Year\"), lit(\"-\"), col(\"Month\"),lit(\"-\"),col(\"Day\")).alias('Date'),'Time','No of Cas',col('Sex').alias(\"Gender\"),'Age','Licence Type')\n",
    "#dataframe is ordered by the date\n",
    "crashes_by_unit_df = crashes_by_unit_df.orderBy(crashes_by_unit_df.Date.asc())\n",
    "crashes_by_unit_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SQL Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a sql view is created for both crashes_df as well as units_df\n",
    "crashes_df.createOrReplaceTempView(\"sql_crashes\")\n",
    "units_df.createOrReplaceTempView(\"sql_units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------+-----------------+-------+-------+---+------------+\n",
      "|Date                    |Time    |Total_Casualities|Unit No|Gender |Age|Licence Type|\n",
      "+------------------------+--------+-----------------+-------+-------+---+------------+\n",
      "|2019-September-Wednesday|11:40 am|0                |2      |Unknown|XXX|Unknown     |\n",
      "|2019-September-Wednesday|11:40 am|0                |1      |Male   |056|Full        |\n",
      "|2019-September-Wednesday|11:38 am|1                |2      |Male   |073|null        |\n",
      "|2019-September-Wednesday|11:38 am|1                |1      |Male   |029|Full        |\n",
      "|2019-September-Wednesday|07:55 pm|1                |1      |Female |031|null        |\n",
      "|2019-September-Wednesday|07:55 pm|1                |2      |Unknown|XXX|null        |\n",
      "|2019-September-Wednesday|06:50 pm|0                |1      |Female |075|Full        |\n",
      "|2019-September-Wednesday|06:50 pm|0                |2      |Male   |039|Full        |\n",
      "|2019-September-Wednesday|06:00 pm|1                |1      |Female |038|null        |\n",
      "|2019-September-Wednesday|06:00 pm|1                |2      |Unknown|XXX|null        |\n",
      "|2019-September-Wednesday|04:36 pm|2                |1      |Male   |028|null        |\n",
      "|2019-September-Wednesday|04:36 pm|2                |2      |Female |033|null        |\n",
      "|2019-September-Wednesday|04:10 pm|0                |2      |Male   |050|Full        |\n",
      "|2019-September-Wednesday|04:10 pm|0                |1      |Female |029|Full        |\n",
      "|2019-September-Wednesday|03:30 pm|0                |2      |Unknown|XXX|null        |\n",
      "|2019-September-Wednesday|03:30 pm|0                |1      |Male   |074|Full        |\n",
      "|2019-September-Wednesday|03:05 pm|0                |2      |Male   |XXX|null        |\n",
      "|2019-September-Wednesday|03:05 pm|0                |1      |Male   |045|Full        |\n",
      "|2019-September-Wednesday|02:20 pm|1                |2      |Male   |040|null        |\n",
      "|2019-September-Wednesday|02:20 pm|1                |1      |Female |023|Unknown     |\n",
      "+------------------------+--------+-----------------+-------+-------+---+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 6.49 ms, sys: 467 µs, total: 6.96 ms\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Join sql_crashes and sql_units on report_id, filter the rows with suburb as Adelaide and select the gender, age, licence type,number of casualities and order by the date\n",
    "sql_crashes_units = spark.sql('''select concat(c.Year,\"-\",c.Month,\"-\",c.Day) as Date, c.Time, c.`Total Cas` as Total_Casualities, u.`Unit No`, u.Sex as Gender, u.Age, u.`Licence Type` \n",
    "                    from sql_crashes c JOIN sql_units u\n",
    "                    on c.Report_ID = u.Report_ID\n",
    "                    where c.Suburb = \"ADELAIDE\"\n",
    "                    Group By c.Year, c.Month, c.Day, c.Time, u.`Unit No`,u.Sex, u.Age, u.`Licence Type`, Total_Casualities\n",
    "                    order by Date desc, Time desc''')\n",
    "sql_crashes_units.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Find the total number of casualties for each suburb when the vehicle was driven by an unlicensed driver. You are required to display the name of the suburb and the total number of casualties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RDD Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tssplit import tssplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 149 ms, sys: 21.4 ms, total: 170 ms\n",
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ADELAIDE', 19),\n",
       " ('DRY CREEK', 18),\n",
       " ('SALISBURY', 18),\n",
       " ('SALISBURY EAST', 16),\n",
       " ('PROSPECT', 14),\n",
       " ('NORTH ADELAIDE', 13),\n",
       " ('ANDREWS FARM', 12),\n",
       " ('ENFIELD', 12),\n",
       " ('INGLE FARM', 11),\n",
       " ('SALISBURY SOUTH', 11)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "crash_rdd2 = crashes_rdd.map(lambda x: tssplit(x, quote='\"', delimiter=',')).\\\n",
    "   filter(lambda x: x[0] != 'REPORT_ID')\n",
    "units_cas = units_rdd_1.filter(lambda x: x[11]=='Unlicenced').map(lambda x: (x[0], x[11]))\n",
    "crashes_cas = crash_rdd2.map(lambda x: (x[0], [x[2],x[6]]))\n",
    "suburb_casualities = crashes_cas.join(units_cas)\n",
    "\n",
    "suburb_casualities = suburb_casualities.map(lambda x: (x[1][0][0],int(x[1][0][1]))).groupByKey().mapValues(list).map(lambda x: (x[0], sum([int(i) for i in x[1]])))\n",
    "suburb_casualities.sortBy(lambda x:-x[1]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataframe Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 2.11 ms, total: 16.6 ms\n",
      "Wall time: 288 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#the crashes_df and units_df are joined on the Report_ID column\n",
    "crashes_units_df = crashes_df.join(units_df,units_df['Report_ID']==crashes_df['Report_ID'])\n",
    "import pyspark.sql.functions as F\n",
    "#The joined dataframe is filtered for the unlicenced licence type and then aggregated by suburb and then the sum of casualities \n",
    "#is calculated for each suburb and the data is sorted in descending order.\n",
    "agg_attribute = 'Suburb'\n",
    "sum_attribute = 'Total Cas'\n",
    "crashes_units_cas_df = crashes_units_df.filter(col(\"Licence Type\") == 'Unlicenced').groupby(agg_attribute)\\\n",
    "                        .agg(F.sum(sum_attribute).alias('Total_Casualities')).sort('Total_Casualities', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|         Suburb|Total_Casualities|\n",
      "+---------------+-----------------+\n",
      "|       ADELAIDE|               19|\n",
      "|      DRY CREEK|               18|\n",
      "|      SALISBURY|               18|\n",
      "| SALISBURY EAST|               16|\n",
      "|       PROSPECT|               14|\n",
      "| NORTH ADELAIDE|               13|\n",
      "|   ANDREWS FARM|               12|\n",
      "|        ENFIELD|               12|\n",
      "|SALISBURY DOWNS|               11|\n",
      "|   BEDFORD PARK|               11|\n",
      "+---------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crashes_units_cas_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SQL Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a sql view is created for both crashes_df as well as units_df\n",
    "crashes_df.createOrReplaceTempView(\"sql_crashes\")\n",
    "units_df.createOrReplaceTempView(\"sql_units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 148 µs, sys: 4.31 ms, total: 4.46 ms\n",
      "Wall time: 115 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#The sql_crashes and sql_units are joined on Report_ID and then filtered for the unlicenced licence type and then grouped by \n",
    "#suburb and then the casualities are calculated for each suburb and the data is sorted in descending order.\n",
    "\n",
    "sql_casualities = spark.sql('''\n",
    "  SELECT c.Suburb,sum(c.`Total Cas`) as Total_Casualities\n",
    "  FROM sql_crashes c JOIN sql_units u\n",
    "  ON c.REPORT_ID = u.REPORT_ID \n",
    "  where u.`Licence Type`= 'Unlicenced' \n",
    "  group by c.Suburb\n",
    "  order by Total_Casualities desc\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|         Suburb|Total_Casualities|\n",
      "+---------------+-----------------+\n",
      "|       ADELAIDE|               19|\n",
      "|      SALISBURY|               18|\n",
      "|      DRY CREEK|               18|\n",
      "| SALISBURY EAST|               16|\n",
      "|       PROSPECT|               14|\n",
      "| NORTH ADELAIDE|               13|\n",
      "|   ANDREWS FARM|               12|\n",
      "|        ENFIELD|               12|\n",
      "|SALISBURY DOWNS|               11|\n",
      "|     INGLE FARM|               11|\n",
      "+---------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print only top 10 rows\n",
    "sql_casualities.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2.4.1, RDD takes 91.2 ms to complete its operations, Dataframe takes 21 ms and SparkSQL takes 10.6 ms to perform the operations. For 2.4.2, RDD takes 145 ms to complete its operations, Dataframe takes 12.7 ms and SparkSQL takes 3.07 ms to perform the operations. It can be observed that the SQL approach takes the least amount of time in comparison to the RDD Approach and the dataframe Approach. RDDs take the maximum amount of time to perform the operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
